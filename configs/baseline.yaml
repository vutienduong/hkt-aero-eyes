experiment_name: "baseline_siamese"

data:
  root: "data/train"
  samples_dir: "samples"
  annotations_file: "annotations/annotations.json"
  frame_size: [640, 360]
  train_split: "data/splits/train.txt"
  val_split: "data/splits/val.txt"
  batch_size: 512  # Optimized for A100 40GB - uses ~32-35GB (80-90% VRAM)
  num_workers: 4  # A100 Colab has more CPU resources, trying 4 workers for faster data loading
  template_images: ["img_1.jpg", "img_2.jpg", "img_3.jpg"]

model:
  backbone: "resnet18"
  backbone_pretrained: true
  feature_dim: 256
  head_type: "siam_rpn"
  template_size: [128, 128]

train:
  epochs: 5  # Reduced for faster iteration; increase for final training
  optimizer: "adamw"
  lr: 0.00001  # Target LR after warmup
  lr_warmup_steps: 500  # Warmup from 1e-6 to 1e-5 over 500 steps
  weight_decay: 0.0001
  bbox_loss_weight: 2.0  # Increase bbox loss weight to force more bbox learning
  log_interval: 50
  val_interval: 2  # Validate every 2 epochs to save time
  checkpoint_dir: "checkpoints"
  gradient_accumulation_steps: 1  # No accumulation needed with large batch_size=512

  # Checkpoint settings
  resume_from_checkpoint: "auto"  # "auto" (ask user), "latest" (auto-resume), null (start fresh)
  save_checkpoint_steps: 100  # Save checkpoint every N steps
  keep_last_n_checkpoints: 3  # Only keep last N checkpoints to save disk space

infer:
  confidence_threshold: 0.4
  max_gap: 3
  min_interval_len: 5
